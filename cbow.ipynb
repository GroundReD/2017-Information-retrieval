{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkipGram예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sg_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 파일을 가져옴\n",
    "filename=sg_util.maybe_download()\n",
    "\n",
    "# 파일을 읽어서 모든 단어들을 words에 list 형태로 집어넣음\n",
    "words = sg_util.read_data(filename)\n",
    "vocabulary_size = 50000\n",
    "\n",
    "# data - 전체 문서에서 각 단어들의 id를 list로 나열\n",
    "# count - 각 단어들이 몇번 나왔는지\n",
    "# dicionary와 reverse_dictionary는 id-word를 매핑한 부분\n",
    "data, count, dictionary, reverse_dictionary = sg_util.build_dataset(words,vocabulary_size)\n",
    "\n",
    "# 나중에 embedding을 마치고 테스트 해볼 갯수와 그 index 최댓값\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.randint(1,valid_window, valid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, skip_window, data_index):\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)  # 아무것도 안들어있는 deque를 생성\n",
    "    generated_batch = 0\n",
    "    while True:\n",
    "        if data_index + span > len(data):  # data의 길이에 끝에 다다르면 data의 첫지점으로 되돌아감\n",
    "            data_index = 0\n",
    "        for _ in range(span):  # span만큼 deque에 삽입\n",
    "            buffer.append(data[data_index])\n",
    "            data_index = data_index + 1\n",
    "        data_index = data_index - span + 1  # data index를 원래대로 돌려놓고, 한칸 움직임\n",
    "        for i in range(1,skip_window+1):  # deque에 들어가있는 data로 학습 데이터 생성\n",
    "            batch[generated_batch] = buffer[skip_window]\n",
    "            labels[generated_batch] = buffer[skip_window - i]\n",
    "            generated_batch += 1\n",
    "            batch[generated_batch] = buffer[skip_window]\n",
    "            labels[generated_batch] = buffer[skip_window + i]\n",
    "            generated_batch += 1\n",
    "            if generated_batch == batch_size:\n",
    "                break\n",
    "        if generated_batch == batch_size:\n",
    "            break\n",
    "    return batch, labels, data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "skip_window=2\n",
    "embedding_size=100\n",
    "n_iteration = 100000 #주어졌던 기본 코드는 1000000\n",
    "\n",
    "X = tf.placeholder(tf.int32,[None])\n",
    "y_ = tf.placeholder(tf.int32,(None,1))\n",
    "\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size,embedding_size], -1,1))\n",
    "embedded = tf.nn.embedding_lookup(embeddings,X)\n",
    "weights = tf.Variable(tf.truncated_normal(shape=(vocabulary_size,embedding_size),stddev=1.0/math.sqrt(embedding_size)))\n",
    "bias = tf.zeros(vocabulary_size)\n",
    "\n",
    "loss=tf.reduce_mean(tf.nn.nce_loss(weights=weights, biases=bias, labels=y_, inputs=embedded, num_sampled=64, num_classes=vocabulary_size))\n",
    "optimizer=tf.train.GradientDescentOptimizer(1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    average_loss = 0\n",
    "    data_index = 0\n",
    "    step = 0\n",
    "    for epoch in range(n_iteration):\n",
    "        step += 1\n",
    "\n",
    "        batch_input, batch_output, data_index = generate_batch(batch_size, skip_window, data_index)\n",
    "\n",
    "        _, loss_eval = sess.run([optimizer, loss], feed_dict={X: batch_input, y_: batch_output})\n",
    "        average_loss += loss_eval\n",
    "\n",
    "        if step % 10000 == 0:\n",
    "            average_loss /= 10000\n",
    "            print('average loss at step ' + str(step) + ': ' + str(average_loss) + ' epoch is ' + str(epoch))\n",
    "            sg_util.closest_words(sess, embeddings.eval(), reverse_dictionary, valid_examples)\n",
    "            average_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reverse_dictionary[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 구분선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sg_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 파일을 가져옴\n",
    "filename=sg_util.maybe_download()\n",
    "\n",
    "# 파일을 읽어서 모든 단어들을 words에 list 형태로 집어넣음\n",
    "words = sg_util.read_data(filename)\n",
    "vocabulary_size = 50000\n",
    "\n",
    "# data - 전체 문서에서 각 단어들의 id를 list로 나열\n",
    "# count - 각 단어들이 몇번 나왔는지\n",
    "# dicionary와 reverse_dictionary는 id-word를 매핑한 부분\n",
    "data, count, dictionary, reverse_dictionary = sg_util.build_dataset(words,vocabulary_size)\n",
    "\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.randint(1,valid_window, valid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, skip_window, data_index):\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)  # 아무것도 안들어있는 deque를 생성\n",
    "    generated_batch = 0\n",
    "    while True:\n",
    "        if data_index + span > len(data):  # data의 길이에 끝에 다다르면 data의 첫지점으로 되돌아감\n",
    "            data_index = 0\n",
    "        for _ in range(span):  # span만큼 deque에 삽입\n",
    "            buffer.append(data[data_index])\n",
    "            data_index = data_index + 1\n",
    "        data_index = data_index - span + 1  # data index를 원래대로 돌려놓고, 한칸 움직임\n",
    "        for i in range(1,skip_window+1):  # deque에 들어가있는 data로 학습 데이터 생성\n",
    "            # CBOW 형태로 batch 생성 및 학습\n",
    "            batch[generated_batch] = buffer[skip_window - i]\n",
    "            labels[generated_batch] = buffer[skip_window]\n",
    "            generated_batch += 1\n",
    "            batch[generated_batch] = buffer[skip_window + i]\n",
    "            labels[generated_batch] = buffer[skip_window]\n",
    "            generated_batch += 1\n",
    "            if generated_batch == batch_size:\n",
    "                break\n",
    "        if generated_batch == batch_size:\n",
    "            break\n",
    "    return batch, labels, data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "skip_window=2\n",
    "embedding_size=100\n",
    "n_iteration = 10000\n",
    "\n",
    "X = tf.placeholder(tf.int32,[None])\n",
    "y_ = tf.placeholder(tf.int32,(None,1))\n",
    "\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size,embedding_size], -1,1))\n",
    "embedded = tf.nn.embedding_lookup(embeddings,X)\n",
    "weights = tf.Variable(tf.truncated_normal(shape=(vocabulary_size,embedding_size),stddev=1.0/math.sqrt(embedding_size)))\n",
    "bias = tf.zeros(vocabulary_size)\n",
    "\n",
    "loss=tf.reduce_mean(tf.nn.nce_loss(weights=weights, biases=bias, labels=y_, inputs=embedded, num_sampled=64, num_classes=vocabulary_size))\n",
    "optimizer=tf.train.GradientDescentOptimizer(1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    average_loss = 0\n",
    "    data_index = 0\n",
    "    step = 0\n",
    "    for epoch in range(n_iteration):\n",
    "        step += 1\n",
    "\n",
    "        batch_input, batch_output, data_index = generate_batch(batch_size, skip_window, data_index)\n",
    "\n",
    "        _, loss_eval = sess.run([optimizer, loss], feed_dict={X: batch_input, y_: batch_output})\n",
    "        average_loss += loss_eval\n",
    "\n",
    "        if step % 10000 == 0:\n",
    "            average_loss /= 10000\n",
    "            print('average loss at step ' + str(step) + ': ' + str(average_loss) + ' epoch is ' + str(epoch))\n",
    "            norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings.eval()), 1, keep_dims=True))\n",
    "            normalized_embeddings = embeddings.eval() / norm\n",
    "            # 행렬에 트레이닝 데이터를 지정\n",
    "            valid_embeddings = tf.nn.embedding_lookup(\n",
    "            normalized_embeddings, valid_examples)\n",
    "            similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "            nearests = tf.nn.top_k(similarity,5)[1]     \n",
    "            for word,nearest_words in enumerate(nearests.eval()):\n",
    "                print('nearests to ',reverse_dictionary[valid_examples[word]],' :',end=' ')\n",
    "                for nearest_word in nearest_words[1:]:\n",
    "                    print(reverse_dictionary[nearest_word],end=', ')\n",
    "                print()\n",
    "            average_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nearest_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reverse_dictionary[51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW 예제 - Use List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sg_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "veryfied\n"
     ]
    }
   ],
   "source": [
    "# 파일을 가져옴\n",
    "filename=sg_util.maybe_download()\n",
    "\n",
    "# 파일을 읽어서 모든 단어들을 words에 list 형태로 집어넣음\n",
    "words = sg_util.read_data(filename)\n",
    "vocabulary_size = 50000\n",
    "\n",
    "# data - 전체 문서에서 각 단어들의 id를 list로 나열\n",
    "# count - 각 단어들이 몇번 나왔는지\n",
    "# dicionary와 reverse_dictionary는 id-word를 매핑한 부분\n",
    "data, count, dictionary, reverse_dictionary = sg_util.build_dataset(words,vocabulary_size)\n",
    "\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.randint(1,valid_window, valid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   12],\n",
       "       [   12],\n",
       "       [    6],\n",
       "       [    6],\n",
       "       [  195],\n",
       "       [  195],\n",
       "       [    2],\n",
       "       [    2],\n",
       "       [ 3134],\n",
       "       [ 3134],\n",
       "       [   46],\n",
       "       [   46],\n",
       "       [   59],\n",
       "       [   59],\n",
       "       [  156],\n",
       "       [  156],\n",
       "       [  128],\n",
       "       [  128],\n",
       "       [  742],\n",
       "       [  742],\n",
       "       [  477],\n",
       "       [  477],\n",
       "       [10572],\n",
       "       [10572],\n",
       "       [  134],\n",
       "       [  134],\n",
       "       [    1],\n",
       "       [    1],\n",
       "       [27350],\n",
       "       [27350],\n",
       "       [    2],\n",
       "       [    2],\n",
       "       [    1],\n",
       "       [    1],\n",
       "       [  103],\n",
       "       [  103],\n",
       "       [  855],\n",
       "       [  855],\n",
       "       [    3],\n",
       "       [    3],\n",
       "       [    1],\n",
       "       [    1],\n",
       "       [15068],\n",
       "       [15068],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    2],\n",
       "       [    2],\n",
       "       [    1],\n",
       "       [    1],\n",
       "       [  151],\n",
       "       [  151],\n",
       "       [  855],\n",
       "       [  855],\n",
       "       [ 3581],\n",
       "       [ 3581],\n",
       "       [    1],\n",
       "       [    1],\n",
       "       [  195],\n",
       "       [  195],\n",
       "       [   11],\n",
       "       [   11],\n",
       "       [  191],\n",
       "       [  191],\n",
       "       [   59],\n",
       "       [   59],\n",
       "       [    5],\n",
       "       [    5],\n",
       "       [    6],\n",
       "       [    6],\n",
       "       [10713],\n",
       "       [10713],\n",
       "       [  215],\n",
       "       [  215],\n",
       "       [    7],\n",
       "       [    7],\n",
       "       [ 1325],\n",
       "       [ 1325],\n",
       "       [  105],\n",
       "       [  105],\n",
       "       [  455],\n",
       "       [  455],\n",
       "       [   20],\n",
       "       [   20],\n",
       "       [   59],\n",
       "       [   59],\n",
       "       [ 2732],\n",
       "       [ 2732],\n",
       "       [  363],\n",
       "       [  363],\n",
       "       [    7],\n",
       "       [    7],\n",
       "       [ 3673],\n",
       "       [ 3673],\n",
       "       [    1],\n",
       "       [    1],\n",
       "       [  709],\n",
       "       [  709],\n",
       "       [    2],\n",
       "       [    2]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_batch(batch_size, skip_window, data_index):\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    batch = np.ndarray(shape=(batch_size, skip_window*2), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size,1), dtype=np.int32)\n",
    "    buffer = collections.deque(maxlen=span)  # 아무것도 안들어있는 deque를 생성\n",
    "    generated_batch = 0\n",
    "    while True:\n",
    "        if data_index + span > len(data):  # data의 길이에 끝에 다다르면 data의 첫지점으로 되돌아감\n",
    "            data_index = 0\n",
    "        for _ in range(span):  # span만큼 deque에 삽입\n",
    "            buffer.append(data[data_index])\n",
    "            data_index = data_index + 1\n",
    "        data_index = data_index - span + 1  # data index를 원래대로 돌려놓고, 한칸 움직임\n",
    "        for i in range(1,skip_window+1):  # deque에 들어가있는 data로 학습 데이터 생성\n",
    "            # CBOW 형태로 batch 생성 및 학습\n",
    "            buffer_list = list(buffer)\n",
    "            labels[generated_batch] = buffer_list.pop(skip_window)\n",
    "            batch[generated_batch] = buffer_list\n",
    "            generated_batch += 1\n",
    "            if generated_batch == batch_size:\n",
    "                break\n",
    "        if generated_batch == batch_size:\n",
    "            break\n",
    "    return batch, labels, data_index\n",
    "\n",
    "data_index = 0\n",
    "batch_input, batch_output, data_index = generate_batch(batch_size, skip_window, data_index)\n",
    "batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4, 100)\n",
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "skip_window=2\n",
    "embedding_size=100\n",
    "n_iteration = 100000\n",
    "\n",
    "X = tf.placeholder(tf.int32, shape=[batch_size, skip_window*2])\n",
    "y_ = tf.placeholder(tf.int32,(None,1))\n",
    "\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size,embedding_size], -1,1))\n",
    "#embedded = tf.nn.embedding_lookup(embeddings,X)\n",
    "# embedded는 3차원\n",
    "embedded = tf.nn.embedding_lookup(embeddings,X)\n",
    "print(embedded.shape)\n",
    "embedded = tf.reduce_mean(embedded,1)\n",
    "print(embedded.shape)\n",
    "weights = tf.Variable(tf.truncated_normal(shape=(vocabulary_size,embedding_size),stddev=1.0/math.sqrt(embedding_size)))\n",
    "bias = tf.zeros(vocabulary_size)\n",
    "\n",
    "loss=tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=weights, biases=bias, labels=y_, inputs=embedded, num_sampled=64, num_classes=vocabulary_size))\n",
    "optimizer=tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    average_loss = 0\n",
    "    data_index = 0\n",
    "    step = 0\n",
    "    for epoch in range(n_iteration):\n",
    "        step += 1\n",
    "\n",
    "        batch_input, batch_output, data_index = generate_batch(batch_size, skip_window, data_index)\n",
    "\n",
    "        _, loss_eval = sess.run([optimizer, loss], feed_dict={X: batch_input, y_: batch_output})\n",
    "        average_loss += loss_eval\n",
    "\n",
    "        if step % 10000 == 0:\n",
    "            average_loss /= 10000\n",
    "            print('average loss at step ' + str(step) + ': ' + str(average_loss) + ' epoch is ' + str(epoch))\n",
    "            norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings.eval()), 1, keep_dims=True))\n",
    "            normalized_embeddings = embeddings.eval() / norm\n",
    "            # 행렬에 트레이닝 데이터를 지정\n",
    "            valid_embeddings = tf.nn.embedding_lookup(\n",
    "            normalized_embeddings, valid_examples)            \n",
    "            similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "            nearests = tf.nn.top_k(similarity,5)[1]     \n",
    "            for word,nearest_words in enumerate(nearests.eval()):\n",
    "                print('nearests to ',reverse_dictionary[valid_examples[word]],' :',end=' ')\n",
    "                for nearest_word in nearest_words[1:]:\n",
    "                    print(reverse_dictionary[nearest_word],end=', ')\n",
    "                print()\n",
    "            average_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
